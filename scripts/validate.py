import requests
import concurrent.futures
import os
import time
import logging
import json  # ç¡®ä¿å¯¼å…¥jsonæ¨¡å—
from datetime import datetime

# é…ç½®è¯¦ç»†æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),  # è¾“å‡ºåˆ°æ§åˆ¶å°
        logging.FileHandler('validation.log')  # è¾“å‡ºåˆ°æ—¥å¿—æ–‡ä»¶
    ]
)
logger = logging.getLogger('ProxyValidator')

# é…ç½®å‚æ•°
TEST_URL = "http://www.gstatic.com/generate_204"
TIMEOUT = 5
MAX_WORKERS = 100
DEBUG_LEVEL = 1  # ã€è°ƒè¯•å†…å®¹è¾“å‡ºçº§åˆ«ã€‘ 0: ä»…æ˜¾ç¤ºå…³é”®ä¿¡æ¯å’Œæœ€ç»ˆæŠ¥å‘Š1: æ­£å¸¸æ¨¡å¼ï¼ˆæ¨èï¼‰2: è¯¦ç»†æ¨¡å¼ï¼ˆæ˜¾ç¤ºæ‰€æœ‰é”™è¯¯ï¼‰3: æç«¯æ¨¡å¼ï¼ˆåŒ…å«å †æ ˆè·Ÿè¸ªï¼‰

def load_proxies():
    """åŠ è½½åŸå§‹ä»£ç†åˆ—è¡¨å¹¶è¿›è¡Œåˆæ­¥æ¸…ç†"""
    try:
        logger.info("Starting proxy load process")
        start_time = time.time()
        
        if not os.path.exists('data/raw_ips.txt'):
            logger.error("âŒ raw_ips.txt not found. Run fetch.py first.")
            return []
        
        with open('data/raw_ips.txt') as f:
            raw_lines = f.readlines()
            logger.info(f"Loaded {len(raw_lines)} lines from raw_ips.txt")
            
            # æ¸…ç†æ— æ•ˆè¡Œ
            proxies = []
            for line in raw_lines:
                stripped = line.strip()
                if stripped and ':' in stripped:
                    proxies.append(stripped)
            
            duration = time.time() - start_time
            logger.info(f"âœ… Proxy load completed. Valid entries: {len(proxies)}/{len(raw_lines)}")
            logger.info(f"â±ï¸ Load time: {duration:.2f} seconds")
            
            return proxies
            
    except Exception as e:
        logger.exception(f"ğŸ”¥ Critical error loading proxies: {str(e)}")
        return []

def test_proxy(proxy):
    """æµ‹è¯•å•ä¸ªä»£ç†çš„æœ‰æ•ˆæ€§"""
    test_start = time.time()
    result = {
        "proxy": proxy,
        "status": "failed",
        "error": None,
        "response_time": 0,
        "status_code": None
    }
    
    try:
        # é…ç½®ä»£ç†è®¾ç½®
        proxy_url = f"http://{proxy}"
        proxies = {"http": proxy_url, "https": proxy_url}
        
        if DEBUG_LEVEL >= 3:
            logger.debug(f"ğŸ” Testing proxy: {proxy}")
        
        # å‘é€æµ‹è¯•è¯·æ±‚
        response = requests.get(
            TEST_URL, 
            proxies=proxies, 
            timeout=TIMEOUT,
            headers={"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36"}
        )
        
        # è®°å½•å“åº”ä¿¡æ¯
        result["response_time"] = time.time() - test_start
        result["status_code"] = response.status_code
        
        # éªŒè¯å“åº”
        if response.status_code == 204:
            result["status"] = "success"
            if DEBUG_LEVEL >= 2:
                logger.info(f"âœ… Success: {proxy} responded in {result['response_time']:.2f}s")
            return proxy
        else:
            result["error"] = f"Unexpected status code: {response.status_code}"
            if DEBUG_LEVEL >= 2:
                logger.warning(f"âš ï¸ Failed: {proxy} returned HTTP {response.status_code} in {result['response_time']:.2f}s")
    
    except requests.exceptions.ConnectTimeout:
        result["error"] = "Connection timeout"
        if DEBUG_LEVEL >= 2:
            logger.warning(f"â±ï¸ Timeout: {proxy} failed to connect within {TIMEOUT}s")
    except requests.exceptions.ProxyError as pe:
        result["error"] = f"Proxy error: {str(pe)}"
        if DEBUG_LEVEL >= 2:
            logger.warning(f"ğŸš« Proxy error: {proxy} - {str(pe)}")
    except requests.exceptions.SSLError:
        result["error"] = "SSL certificate error"
        if DEBUG_LEVEL >= 2:
            logger.warning(f"ğŸ”’ SSL error with proxy: {proxy}")
    except Exception as e:
        result["error"] = f"General error: {str(e)}"
        if DEBUG_LEVEL >= 2:
            logger.error(f"ğŸ”¥ Error testing {proxy}: {str(e)}", exc_info=DEBUG_LEVEL >= 3)
    
    # è®°å½•è¯¦ç»†è°ƒè¯•ä¿¡æ¯
    if DEBUG_LEVEL >= 3:
        logger.debug(f"Test result for {proxy}: {result}")
    
    return None

def log_progress(current, total, valid_count, start_time):
    """è®°å½•éªŒè¯è¿›åº¦"""
    elapsed = time.time() - start_time
    processed_percent = (current / total) * 100
    speed = current / elapsed if elapsed > 0 else 0
    
    progress = f"ğŸš€ Progress: {current}/{total} ({processed_percent:.1f}%)"
    stats = f"âœ… Valid: {valid_count} | ï¸ Elapsed: {elapsed:.1f}s |  ğŸ“ˆ Speed: {speed:.1f} proxies/s"
    
    logger.info(progress)
    logger.info(stats)
    logger.info("-" * 60)

if __name__ == "__main__":
    # åˆå§‹åŒ–
    start_time = time.time()
    logger.info("=" * 70)
    logger.info(f"ğŸš€ STARTING PROXY VALIDATION - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info(f"ğŸ”§ Configuration: Test URL={TEST_URL}, Timeout={TIMEOUT}s, Workers={MAX_WORKERS}")
    logger.info("=" * 70)
    
    # åŠ è½½ä»£ç†
    proxies = load_proxies()
    if not proxies:
        logger.error("âŒ No proxies to validate. Exiting.")
        exit(1)
    
    # åˆå§‹åŒ–éªŒè¯
    valid_proxies = []
    total_proxies = len(proxies)
    logger.info(f"ğŸ” Beginning validation of {total_proxies} proxies")
    
    # ä½¿ç”¨çº¿ç¨‹æ± éªŒè¯ä»£ç†
    try:
        with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            futures = {executor.submit(test_proxy, proxy): proxy for proxy in proxies}
            
            for i, future in enumerate(concurrent.futures.as_completed(futures)):
                proxy = futures[future]
                result = future.result()
                
                if result:
                    valid_proxies.append(result)
                    if DEBUG_LEVEL >= 1:
                        logger.info(f"âœ… Added valid proxy: {result}")
                
                # å®šæœŸè®°å½•è¿›åº¦
                if (i + 1) % 100 == 0 or (i + 1) == total_proxies:
                    log_progress(i + 1, total_proxies, len(valid_proxies), start_time)
    
    except Exception as e:
        logger.exception(f"ğŸ”¥ Critical error during validation: {str(e)}")
    
    # ä¿å­˜æœ‰æ•ˆä»£ç†
    os.makedirs('data', exist_ok=True)
    with open('data/valid_ips.txt', 'w') as f:
        f.write('\n'.join(valid_proxies))
    
    # æœ€ç»ˆæŠ¥å‘Šç»Ÿè®¡æ•°æ®æ”¶é›†
    validation_time = time.time() - start_time
    success_rate = (len(valid_proxies) / total_proxies) * 100 if total_proxies > 0 else 0
    
    # å†™å…¥éªŒè¯ç»Ÿè®¡ä¿¡æ¯
    stats = {
        "valid_count": len(valid_proxies),
        "total_count": total_proxies,
        "success_rate": success_rate,
        "validation_time": validation_time,
        "run_timestamp": datetime.now().isoformat()
    }
    
    # ç¡®ä¿statsç›®å½•å­˜åœ¨
    os.makedirs('stats', exist_ok=True)
    with open('stats/validation_stats.json', 'w') as stats_file:
        json.dump(stats, stats_file, indent=2)
    logger.info(f"ğŸ“Š Validation statistics saved to stats/validation_stats.json")
    
    # æœ€ç»ˆæŠ¥å‘Šæ—¥å¿—è¾“å‡º
    logger.info("=" * 70)
    logger.info(f"ğŸ VALIDATION COMPLETED - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info(f"ğŸ“Š Results: {len(valid_proxies)} valid out of {total_proxies} ({success_rate:.2f}% success rate)")
    logger.info(f"â±ï¸ Total time: {validation_time:.2f} seconds")
    logger.info(f"ğŸ“ˆ Average speed: {total_proxies/validation_time:.2f} proxies/second")
    logger.info(f"ğŸ’¾ Valid proxies saved to data/valid_ips.txt")
    logger.info("=" * 70)
    
    # é€€å‡ºçŠ¶æ€
    if valid_proxies:
        logger.info("âœ… Validation successful")
        exit(0)
    else:
        logger.error("âŒ No valid proxies found")
        exit(1)
